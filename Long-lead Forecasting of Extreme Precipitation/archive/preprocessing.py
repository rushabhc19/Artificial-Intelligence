#!/usr/bin/python
"""This module contains methods used to process raw data into the set of
features that will be considered by SAOLA algorithm"""
import gc
import os
from collections import OrderedDict
import pandas as pd

def get_data_v3(skip_days=0, n_days=10):
    """
    Heavy change! Now removed decreased precision. Using default values.
    The reason for this change is that it was hurting the generated CSV
    file size.

    This version of get_data returns a single dataframe instead of a
    dictionary of dataframes. This dataframe will have 47952 columns that are
    referenced with a MultiIndex.

    To select data from this dataframe do this:
    value = df.loc[day_id, ('variablename', location_id)]

    Example:
    value = df.loc[11323, ('z1000', 5328)]

    Correlation between any two columns can be calculated like this:
    correlation = df.loc[:,('pw',1)].corr(df.loc[:,('z500',2)])

    Beware that this is RAM consuming.

    The total amount of days available is 11323
    Calling this method with skip_days=0, n_days=11323 will consume ~5GB RAM

    This method uses the aggregated versions of CSV files. You can use the
    provided bash script to build these files from the originals.
    """

    # Dictionary that will contain data for all 9 variables at every location
    # the set of days will be determined by method parameters
    data = OrderedDict([
        ('pw', None),
        ('t850', None),
        ('u300', None),
        ('u850', None),
        ('v300', None),
        ('v850', None),
        ('z1000', None),
        ('z300', None),
        ('z500', None)
    ])

    print ("Reading CSV files from day %d to %d..."
           % (skip_days + 1, skip_days + n_days))
    for var in data:
        # Read CSVs one by one and store dataframes in dictionary
        print "Reading %s" % var
        data[var] = pd.read_csv(
            "raw_data/aggreg/%s.csv" % var,
            usecols=range(4, 5332),     # 4th col -> location_id=1
            skiprows=skip_days,         # select rows starting from start_day
            nrows=n_days,               # selecting only specified rows
            header=None,                # manually setting col names
            names=range(1, 5329)        # names are simply location_id
        )

    print "Done!"
    print "Now building the dataframe"
    dataframe = pd.concat(data.values(), axis=1, keys=data.keys())
    print "Dataframe is ready..."

    del data
    gc.collect()

    # Shift index
    dataframe.index += skip_days + 1

    return dataframe

def save_dataframe(dataframe, filename='data.pickle'):
    """Saves a pickle with dataframe data"""
    if not os.path.exists('data'):
        os.makedirs('data')
    filepath = os.path.join('data', filename)
    dataframe.to_pickle(filepath)

def load_dataframe(filename='data.pickle'):
    """Loads and returns the dataframe from a saved pickle"""
    filepath = os.path.join('data', filename)
    return pd.read_pickle(filepath)

def set_loc_day_feats(data, location_id, day_id, target_list):
    """Reads the dictionary with all data, and returns the set of 9 variables
    for the given location and day.

    data:           dictionary generated by get_data()
    location_id:    integer. [1 to 5328]
    day_id:         integer. [1 to 11323] (depending of what is avail in data dict)
    target_list:    list of length 9. Values will be replaced
    """

    # using a list to ensure the order of variables
    variables = ['pw', 't850', 'u300', 'u850', 'v300',
                 'v850', 'z1000', 'z300', 'z500']

    for i, var in enumerate(variables):
        target_list[i] = data[var][location_id][day_id]

    return

def get_locations():
    """Returns DataFrame with coordinates of every location.
    Location ID matches the IDs at other CSV files"""
    # IOWA ID 1426
    return pd.read_csv("locations_coordinates.csv").set_index("ID")

def build_iowa_data():
    """Builds a dataframe with precipitation data for Iowa. The resulting
    dataframe also includes a column with the sum of the next 15 days of
    precipitation for each day, and a column with class labels that are
    calculated according to sum15 column.
    """

    print "reading Iowa data..."

    # read the csv
    iowa = pd.read_csv(
        'raw_data/aggreg/iowa.csv',
        usecols=[4],
        header=None,
        names=['precip']
    )

    # this list will be used to build the new sum15 column for dataframe
    sum15s = []

    # iterate over dataframe
    for index in range(len(iowa) - 14):
        # initialize ith element of list
        sum15s.append(0.0)

        # iterate over the next 15 rows. We need to cap the upper bound
        # of the range so we dont overflow
        for j in range(index, index + 15):
            sum15s[index] += iowa['precip'][j]

    # extend the dataframe to include the new column
    iowa = iowa.assign(sum15=pd.Series(sum15s))

    # we now want to build class column.
    # finding threshold for 95 percentile
    threshold = iowa.quantile(q=0.95, axis=0)['sum15']

    # list used to build the column
    class_labels = []

    # iterate over dataframe
    for index in range(len(iowa)):
        if iowa['sum15'][index] >= threshold:
            # found a flood, this is a positive label
            class_labels.append(int(1))
        elif iowa['sum15'][index] < threshold:
            # negative label
            class_labels.append(int(0))
        else:
            # undefined label
            class_labels.append(int(-1))

    iowa = iowa.assign(label=pd.Series(class_labels, dtype='int8'))

    iowa.index += 1

    return iowa

def build_features(data, iowa, day_id):
    """get features for one day returns 1D list of 10*9*5328 + 1
    the first day we can work with id 10th day
    last day is 11304
    Jan 10th 1980 - Dec 11th 2010
    """

    return pd.concat(
        [
            data.loc[day_id - 9],
            data.loc[day_id - 8],
            data.loc[day_id - 7],
            data.loc[day_id - 6],
            data.loc[day_id - 5],
            data.loc[day_id - 4],
            data.loc[day_id - 3],
            data.loc[day_id - 2],
            data.loc[day_id - 1],
            data.loc[day_id - 0],
            pd.Series(
                [iowa.loc[day_id + 5, 'label']],    # The class label {0, 1}
                index=[['label'], [7000]],          # Defining index for labels
                                                    # needed to match other cols
                dtype='int8',
                copy=False
            )
        ],
        keys=['d-9', 'd-8', 'd-7', 'd-6', 'd-5',            # New layer for
              'd-4', 'd-3', 'd-2', 'd-1', 'd-0', 'd+5'],    # MultiIndex
        copy=False
    )

def build_data_csv_file():
    """Writes the CSV. It works, but I think we need to find a better way.
    It takes a few minutes to read a single column from the resulting 32GB file
    """

    # read necessary data
    iowa = build_iowa_data()

    # Initialize variables
    data = None             # data will be read from within loop
    skip_days = 0           # will update when requiring new set of data
    n_days = 200 + 10       # will process data in batches of size 200
    last_day_available = 0  # will be determined after reading data

    # make sure data folder is available
    if not os.path.exists('data'):
        os.makedirs('data')

    # build the string with path to output file
    filepath = os.path.join('data', 'output.csv')
    output_f = open(filepath, 'w')

    print "Now writing the output CSV file...\n\n"
    # we start at day 10 and exclude the last 19 days.
    # +1 makes sure the last valid day gets included!
    for day_id in range(10, 11323-14-5 +1):
        # check if required data is available for current day_id
        if day_id > last_day_available:
            # free memory
            del data

            # read necessary data
            data = get_data_v3(skip_days=skip_days, n_days=n_days)

            # update status variables
            last_day_available = skip_days + n_days
            skip_days = last_day_available - 10
            print "Last day avail in this batch: %d\n\n" % last_day_available

        print "\033[FWriting features and class label for day % 6d" % day_id

        # build single big row as a dataframe so we can easily append to file
        dataframe = pd.concat([build_features(data, iowa, day_id)],
                              join='outer', axis=1, copy=False).T

        # fix index!
        dataframe.index += day_id

        # append to output file
        dataframe.to_csv(output_f, header=False)

        # free some memory
        del dataframe

    print "\n\nDone!!!"

def examples():
    """The examples method. This will be executed when calling the script
    directly"""
    # coord contains latitude and longitude values for each location
    coordinates = get_locations()

    # features data for all locations
    data = get_data_v3(skip_days=0, n_days=10)

    # Example
    loc_id = 1
    day_id = 1
    coord = coordinates.loc[loc_id]
    print "Coordinates:"
    print "Lat:\t%.2f\tLon:\t%.2f" % (coord['lat'], coord['lon'])
    print "Day ID:\t%d\tLoc ID:\t%d\n" % (day_id, loc_id)

    # Fill values of m_s_t with data from loc_id=2 and day_id=5
    # Initialize vector of length 9
    #m_s_t = [None, None, None, None, None, None, None, None, None]
    #set_loc_day_feats(data, loc_id, day_id, m_s_t)

    variables = ['pw', 't850', 'u300', 'u850', 'v300',
                 'v850', 'z1000', 'z300', 'z500']

    # recovering this list from the DataFrame is easy:
    print data.loc[day_id, (variables, loc_id)]
    print

    # and we can make it a list
    m_s_t = list(data.loc[day_id, (variables, loc_id)])

    # Print m_s_t to check
    print ("This is the list of variables "
           "for loc_id: %d, day_id: %d") % (loc_id, day_id)
    print m_s_t

    print "\nFeatures for loc_id: %d, day_id: %d" % (loc_id, day_id)

    for i, var in enumerate(variables):
        print "\t%s:\t%s" % (var, m_s_t[i])

if __name__ == "__main__":
    pass
    #examples()
    #build_data_csv_file()
    #cols = pd.read_csv('data/output.csv', usecols=[0, 1, 479521],
    #                   header=None, index_col=0)

    #print cols
